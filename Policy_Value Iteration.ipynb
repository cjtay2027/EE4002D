{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymdptoolbox --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mdptoolbox\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1,2,3],[4,2,5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmd0 = 0.013364\n",
    "lmd1 = 0.333442\n",
    "lmdM = 1 - lmd0 - lmd1 #0.6531...\n",
    "mu0 = 0.125\n",
    "mu1 = 0.25\n",
    "muM = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = ['no maintenance','maintenance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0_tm = np.array([[lmdM, lmd1, 0, 0, 0, 0, 0, 0, 0, lmd0], #current state 0 to next state\n",
    "                  [0, lmdM, lmd1, 0, 0, 0, 0, 0, 0, lmd0], #current state 1 to next state\n",
    "                  [0, 0, lmdM, lmd1, 0, 0, 0, 0, 0, lmd0], #current state 2 to next state\n",
    "                  [0, 0, 0, lmdM, 0, 0, 0, 0, lmd1, lmd0], #current state 3 to next state\n",
    "                  [muM, 0, 0, 0, 1-muM, 0, 0, 0, 0, 0], #current state 4 to next state\n",
    "                  [muM, 0, 0, 0, 0, 1-muM, 0, 0, 0, 0], #current state 5 to next state\n",
    "                  [0, muM, 0, 0, 0, 0, 1-muM, 0, 0, 0], #current state 6 to next state\n",
    "                  [0, 0, muM, 0, 0, 0, 0, 1-muM, 0, 0], #current state 7 to next state\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], #current state 8 to next state\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]) #current state 9 to next state\n",
    "\n",
    "\n",
    "#transition matrix for a = 1 (maintenance steps)\n",
    "a1_tm = np.array([[0, 0, 0, 0, 1-lmd0, 0, 0, 0, 0, lmd0], #current state 0 to next state\n",
    "                  [0, 0, 0, 0, 0, 1-lmd0, 0, 0, 0, lmd0], #current state 1 to next state\n",
    "                  [0, 0, 0, 0, 0, 0, 1-lmd0, 0, 0, lmd0], #current state 2 to next state\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 1-lmd0, 0, lmd0], #current state 3 to next state\n",
    "                  [muM, 0, 0, 0, 1-muM, 0, 0, 0, 0, 0], #current state 4 to next state\n",
    "                  [muM, 0, 0, 0, 0, 1-muM, 0, 0, 0, 0], #current state 5 to next state\n",
    "                  [0, muM, 0, 0, 0, 0, 1-muM, 0, 0, 0], #current state 6 to next state\n",
    "                  [0, 0, muM, 0, 0, 0, 0, 1-muM, 0, 0], #current state 7 to next state\n",
    "                  [mu1, 0, 0, 0, 0, 0, 0, 0, 1-mu1, 0], #current state 8 to next state\n",
    "                  [mu0, 0, 0, 0, 0, 0, 0, 0, 0, 1-mu0]]) #current state 9 to next state\n",
    "\n",
    "tm = [a0_tm,a1_tm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = np.stack((a0_tm,a1_tm))\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  300.        ,  -750.        ],\n",
       "       [  233.33333333,  -750.        ],\n",
       "       [  100.        ,  -750.        ],\n",
       "       [-1166.66666667,  -750.        ],\n",
       "       [  250.        ,   250.        ],\n",
       "       [  250.        ,   250.        ],\n",
       "       [  200.        ,   200.        ],\n",
       "       [  150.        ,   150.        ],\n",
       "       [-3000.        ,  1000.        ],\n",
       "       [-1000.        ,  1000.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_main = {0:[4,9],1:[5,9],2:[6,9],3:[7,9],4:[4,0],5:[5,0],6:[6,1],7:[7,2],8:[0],9:[0]}\n",
    "trans_no_main = {0:[0,1,9],1:[1,2,9],2:[2,3,9],3:[3,8,9],4:[4,0],5:[5,0],6:[6,1],7:[7,2],8:[8],9:[9]}\n",
    "reward_func = {0:1000,1:900,2:800,3:500,4:-500,5:-500,6:-500,7:-500,8:-3000,9:-1000}\n",
    "# maintenance_cost = 1000\n",
    "# reward_func = {0:2000,1:1500,2:1000,3:500,4:-maintenance_cost,5:-maintenance_cost,6:-maintenance_cost,7:-maintenance_cost,8:-3000,9:-2000}\n",
    "\n",
    "def exp_reward(trans,reward_func):\n",
    "    reward = []\n",
    "    \n",
    "    for row,col in trans.items():\n",
    "        total = 0\n",
    "        for j in col:\n",
    "            total+=reward_func[j]\n",
    "            \n",
    "        reward.append(total/len(col))\n",
    "    return reward\n",
    "\n",
    "R1 = np.array(exp_reward(trans_main,reward_func))\n",
    "R1 = np.expand_dims(R1,1)\n",
    "R0 = np.array(exp_reward(trans_no_main,reward_func))\n",
    "R0 = np.expand_dims(R0,1)\n",
    "R = np.hstack((R0,R1))\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.8 #discount factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value Iteration\n",
    "\n",
    "vi = mdptoolbox.mdp.ValueIteration(P, R, epsilon)\n",
    "vi.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, 1, 0, 0, 0, 0, 1, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1031.8835814615386,\n",
       " 574.9328528534326,\n",
       " 6.984952861149225,\n",
       " -509.71090816181413,\n",
       " 1104.5687872013461,\n",
       " 1104.5687872013461,\n",
       " 716.6017778349459,\n",
       " 254.63660633399166,\n",
       " 3015.9113625184646,\n",
       " 3677.2487109094923)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi.iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Policy iteration\n",
    "\n",
    "PI = mdptoolbox.mdp.PolicyIteration(P, R, epsilon)\n",
    "#PI.setVerbose()\n",
    "PI.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI.iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q learning\n",
    "Ql =  mdptoolbox.mdp.QLearning(P, R, epsilon)\n",
    "Ql.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ql.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ql.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Ql.policy)):\n",
    "    print(f\"Best action in state {i} is {action[Ql.policy[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Rewards over time following this policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pol = vi.policy\n",
    "rewards = [0]\n",
    "r = 0\n",
    "timesteps = 3000\n",
    "state = 0\n",
    "\n",
    "for i in range(timesteps-1):\n",
    "    action = pol[state]\n",
    "    transition_mat_action = tm[action]\n",
    "    nxt_state = np.random.choice([i for i in range(10)],1,p=transition_mat_action[state])[0] #transition\n",
    "    r = reward_func[nxt_state]\n",
    "    rewards.append(rewards[-1]+r) #cumulative rewards\n",
    "    state = nxt_state\n",
    "    \n",
    "t = [i for i in range(timesteps)]\n",
    "fig = plt.figure(figsize= (10,10))\n",
    "plt.plot(t, rewards)\n",
    "plt.title(\"Value Iteration\")\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.xlabel('Timestep')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average across 20 timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine import Machine\n",
    "import numpy as np\n",
    "#Parameters\n",
    "lmd0 = 0.013364\n",
    "lmd1 = 0.333442\n",
    "lmdM = 1 - lmd0 - lmd1 #0.6531...\n",
    "mu0 = 0.125\n",
    "mu1 = 0.25\n",
    "muM = 0.5\n",
    "maintenance_cost = 750\n",
    "\n",
    "\n",
    "#transition matrices\n",
    "\n",
    "#transition matrix for a = 0 (no maintenance)\n",
    "a0_tm = np.array([[lmdM, lmd1, 0, 0, 0, 0, 0, 0, 0, lmd0], #current state 0 to next state\n",
    "                  [0, lmdM, lmd1, 0, 0, 0, 0, 0, 0, lmd0], #current state 1 to next state\n",
    "                  [0, 0, lmdM, lmd1, 0, 0, 0, 0, 0, lmd0], #current state 2 to next state\n",
    "                  [0, 0, 0, lmdM, 0, 0, 0, 0, lmd1, lmd0], #current state 3 to next state\n",
    "                  [muM, 0, 0, 0, 1-muM, 0, 0, 0, 0, 0], #current state 4 to next state\n",
    "                  [muM, 0, 0, 0, 0, 1-muM, 0, 0, 0, 0], #current state 5 to next state\n",
    "                  [0, muM, 0, 0, 0, 0, 1-muM, 0, 0, 0], #current state 6 to next state\n",
    "                  [0, 0, muM, 0, 0, 0, 0, 1-muM, 0, 0], #current state 7 to next state\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], #current state 8 to next state\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]) #current state 9 to next state\n",
    "\n",
    "\n",
    "#transition matrix for a = 1 (maintenance steps)\n",
    "a1_tm = np.array([[0, 0, 0, 0, 1-lmd0, 0, 0, 0, 0, lmd0], #current state 0 to next state\n",
    "                  [0, 0, 0, 0, 0, 1-lmd0, 0, 0, 0, lmd0], #current state 1 to next state\n",
    "                  [0, 0, 0, 0, 0, 0, 1-lmd0, 0, 0, lmd0], #current state 2 to next state\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 1-lmd0, 0, lmd0], #current state 3 to next state\n",
    "                  [muM, 0, 0, 0, 1-muM, 0, 0, 0, 0, 0], #current state 4 to next state\n",
    "                  [muM, 0, 0, 0, 0, 1-muM, 0, 0, 0, 0], #current state 5 to next state\n",
    "                  [0, muM, 0, 0, 0, 0, 1-muM, 0, 0, 0], #current state 6 to next state\n",
    "                  [0, 0, muM, 0, 0, 0, 0, 1-muM, 0, 0], #current state 7 to next state\n",
    "                  [mu1, 0, 0, 0, 0, 0, 0, 0, 1-mu1, 0], #current state 8 to next state\n",
    "                  [mu0, 0, 0, 0, 0, 0, 0, 0, 0, 1-mu0]]) #current state 9 to next state\n",
    "tm = [a0_tm,a1_tm]\n",
    "r_func = {0:2000,1:1500,2:1000,3:500,4:-maintenance_cost,5:-maintenance_cost,6:-maintenance_cost,7:-maintenance_cost,8:-3000,9:-2000}\n",
    "\n",
    "class MachineEnv():\n",
    "    def __init__(self,tm,r_func):\n",
    "        self.action_space = [0,1]\n",
    "        self.state = 0 #Random initialise the start state, assumes uniform distribution for initial state,random.randrange(10)\n",
    "        self.state_seq = [] #initialise a list that records the actual states\n",
    "        self.reward_func = r_func\n",
    "        self.transition  = tm\n",
    "        self.steps = 0\n",
    "        self.done = False\n",
    "    \n",
    "    def step(self,action):\n",
    "        transition_mat_action = self.transition[action]\n",
    "        nxt_state = np.random.choice([i for i in range(10)],1,p=transition_mat_action[self.state])[0] #select nxt state based\n",
    "        reward = self.reward_func[nxt_state]\n",
    "        self.state = nxt_state #update state\n",
    "        self.steps +=1\n",
    "        \n",
    "        if(((nxt_state == 0) and (self.steps > 20)) or (self.steps >= 50)):#condition for end of episode\n",
    "            self.done = True\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        self.done= False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_avg_return(environment, policy, steps,num_episodes=10):\n",
    "#     total_return = 0.0\n",
    "#     for _ in range(num_episodes):\n",
    "#         environment.reset()\n",
    "#         episode_return = 0.0 \n",
    "#         for _ in range(steps):\n",
    "#             action_step = policy[environment.state]\n",
    "#             episode_return += environment.step(action_step)\n",
    "#         total_return += episode_return   \n",
    "#     avg_return = total_return / num_episodes\n",
    "#     return avg_return# Evaluate the agent's policy once before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episodic Average Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy,num_episodes):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        environment.reset()\n",
    "        episode_return = 0.0 \n",
    "        while(not machine.done):\n",
    "            action_step = policy[environment.state]\n",
    "            episode_return += environment.step(action_step)\n",
    "        total_return += episode_return \n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine = MachineEnv(tm,r_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ep = 40\n",
    "trained = []\n",
    "iteration = [i for i in range(ep)]\n",
    "\n",
    "for i in range(ep):\n",
    "    print(f\"Trial {i}\")\n",
    "    x2=compute_avg_return(machine, vi.policy, 30)\n",
    "    trained.append(x2)\n",
    "    \n",
    "fig = plt.figure(figsize = (8,8))\n",
    "plt.plot(iteration,trained)\n",
    "plt.title(\"Average return over 1 episode\")\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Average Return')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
