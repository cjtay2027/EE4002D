{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0b6a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy,MlpLstmPolicy\n",
    "from stable_baselines import ACKTR\n",
    "import tensorflow as tf\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from machine import Machine\n",
    "from GymMachEnv import MachineEnv\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "from stable_baselines.common import make_vec_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6408d4",
   "metadata": {},
   "source": [
    "# Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4191d635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\overl\\anaconda3\\lib\\site-packages\\stable_baselines\\common\\base_class.py:1143: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\overl\\anaconda3\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:502: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "---------------------------------\n",
      "| ep_len_mean        | 13.7     |\n",
      "| ep_reward_mean     | 13.7     |\n",
      "| explained_variance | -0.0109  |\n",
      "| fps                | 140      |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 0.693    |\n",
      "| policy_loss        | 5.08     |\n",
      "| total_timesteps    | 80       |\n",
      "| value_loss         | 75       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| ep_len_mean        | 72.6     |\n",
      "| ep_reward_mean     | 72.6     |\n",
      "| explained_variance | 0.000732 |\n",
      "| fps                | 3176     |\n",
      "| nupdates           | 100      |\n",
      "| policy_entropy     | 0.548    |\n",
      "| policy_loss        | 4.01     |\n",
      "| total_timesteps    | 8000     |\n",
      "| value_loss         | 62.5     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| ep_len_mean        | 138      |\n",
      "| ep_reward_mean     | 138      |\n",
      "| explained_variance | -0.0042  |\n",
      "| fps                | 3814     |\n",
      "| nupdates           | 200      |\n",
      "| policy_entropy     | 0.592    |\n",
      "| policy_loss        | 3.13     |\n",
      "| total_timesteps    | 16000    |\n",
      "| value_loss         | 49.2     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| ep_len_mean        | 172      |\n",
      "| ep_reward_mean     | 172      |\n",
      "| explained_variance | 0.00032  |\n",
      "| fps                | 4172     |\n",
      "| nupdates           | 300      |\n",
      "| policy_entropy     | 0.517    |\n",
      "| policy_loss        | 3.37     |\n",
      "| total_timesteps    | 24000    |\n",
      "| value_loss         | 47.1     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = make_vec_env('CartPole-v0', n_envs=4)\n",
    "\n",
    "model = ACKTR(MlpPolicy, env, verbose=1,tensorboard_log=\"./ACKTR_cartpole_tensorboard/\")\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"acktr_cartpole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf0bf2",
   "metadata": {},
   "source": [
    "# GMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c12fa58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\overl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator GaussianMixture from version 0.20.1 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "machine = Machine()\n",
    "machine.curr_state = 0\n",
    "env = DummyVecEnv([lambda: MachineEnv(machine)])\n",
    "machine2 = Machine()\n",
    "machine2.curr_state = 0\n",
    "eval_env = DummyVecEnv([lambda: MachineEnv(machine2)])\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./best/trial2',\n",
    "                             log_path='./best/trial2', eval_freq=500,\n",
    "                             deterministic=True, render=False)\n",
    "#model = ACKTR('MlpPolicy',env,verbose=1, tensorboard_log=\"./ACKTR_GMM_tensorboard/\").learn(total_timesteps=100000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d31ee9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| explained_variance | -5.36e-06 |\n",
      "| fps                | 36        |\n",
      "| nupdates           | 1         |\n",
      "| policy_entropy     | 0.693     |\n",
      "| policy_loss        | 494       |\n",
      "| total_timesteps    | 20        |\n",
      "| value_loss         | 4.02e+06  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=500, episode_reward=3800.00 +/- 2638.18\n",
      "Episode length: 82.00 +/- 10.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-8100.00 +/- 7838.37\n",
      "Episode length: 60.60 +/- 37.35\n",
      "Eval num_timesteps=1500, episode_reward=6000.00 +/- 4940.04\n",
      "Episode length: 11.40 +/- 6.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=3580.00 +/- 2442.46\n",
      "Episode length: 9.40 +/- 3.38\n",
      "---------------------------------\n",
      "| explained_variance | 0.000151 |\n",
      "| fps                | 203      |\n",
      "| nupdates           | 100      |\n",
      "| policy_entropy     | 0.574    |\n",
      "| policy_loss        | 1.08e+03 |\n",
      "| total_timesteps    | 2000     |\n",
      "| value_loss         | 1.92e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=2080.00 +/- 1803.77\n",
      "Episode length: 7.40 +/- 2.15\n",
      "Eval num_timesteps=3000, episode_reward=8560.00 +/- 6733.08\n",
      "Episode length: 15.20 +/- 8.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3500, episode_reward=8980.00 +/- 3876.80\n",
      "Episode length: 15.20 +/- 4.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=3800.00 +/- 2746.63\n",
      "Episode length: 9.80 +/- 3.19\n",
      "---------------------------------\n",
      "| explained_variance | 0.000633 |\n",
      "| fps                | 230      |\n",
      "| nupdates           | 200      |\n",
      "| policy_entropy     | 0.514    |\n",
      "| policy_loss        | 310      |\n",
      "| total_timesteps    | 4000     |\n",
      "| value_loss         | 5.91e+06 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=2200.00 +/- 812.40\n",
      "Episode length: 7.20 +/- 1.72\n",
      "Eval num_timesteps=5000, episode_reward=24740.00 +/- 18683.96\n",
      "Episode length: 50.00 +/- 33.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5500, episode_reward=2580.00 +/- 1757.73\n",
      "Episode length: 8.20 +/- 2.32\n",
      "Eval num_timesteps=6000, episode_reward=5020.00 +/- 2228.36\n",
      "Episode length: 11.20 +/- 2.86\n",
      "---------------------------------\n",
      "| explained_variance | 0.000373 |\n",
      "| fps                | 232      |\n",
      "| nupdates           | 300      |\n",
      "| policy_entropy     | 0.584    |\n",
      "| policy_loss        | 1.31e+03 |\n",
      "| total_timesteps    | 6000     |\n",
      "| value_loss         | 1.62e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=4780.00 +/- 1680.95\n",
      "Episode length: 10.40 +/- 1.85\n",
      "Eval num_timesteps=7000, episode_reward=6220.00 +/- 4068.61\n",
      "Episode length: 12.40 +/- 4.50\n",
      "Eval num_timesteps=7500, episode_reward=2820.00 +/- 1397.71\n",
      "Episode length: 8.00 +/- 1.41\n",
      "Eval num_timesteps=8000, episode_reward=4660.00 +/- 1502.80\n",
      "Episode length: 10.60 +/- 2.42\n",
      "---------------------------------\n",
      "| explained_variance | 0.0011   |\n",
      "| fps                | 240      |\n",
      "| nupdates           | 400      |\n",
      "| policy_entropy     | 0.655    |\n",
      "| policy_loss        | 3.94e+03 |\n",
      "| total_timesteps    | 8000     |\n",
      "| value_loss         | 6.23e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=10040.00 +/- 7341.28\n",
      "Episode length: 15.20 +/- 6.40\n",
      "Eval num_timesteps=9000, episode_reward=4240.00 +/- 2279.12\n",
      "Episode length: 10.60 +/- 2.65\n",
      "Eval num_timesteps=9500, episode_reward=6720.00 +/- 2342.14\n",
      "Episode length: 12.80 +/- 3.19\n",
      "Eval num_timesteps=10000, episode_reward=5900.00 +/- 3706.21\n",
      "Episode length: 12.00 +/- 4.24\n",
      "----------------------------------\n",
      "| explained_variance | -0.000358 |\n",
      "| fps                | 243       |\n",
      "| nupdates           | 500       |\n",
      "| policy_entropy     | 0.642     |\n",
      "| policy_loss        | 2.65e+03  |\n",
      "| total_timesteps    | 10000     |\n",
      "| value_loss         | 2.57e+07  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=2940.00 +/- 2947.95\n",
      "Episode length: 9.00 +/- 3.46\n",
      "Eval num_timesteps=11000, episode_reward=3840.00 +/- 2729.54\n",
      "Episode length: 9.60 +/- 3.26\n",
      "Eval num_timesteps=11500, episode_reward=2320.00 +/- 2405.33\n",
      "Episode length: 7.20 +/- 3.71\n",
      "Eval num_timesteps=12000, episode_reward=3860.00 +/- 3815.55\n",
      "Episode length: 9.00 +/- 5.18\n",
      "---------------------------------\n",
      "| explained_variance | 0.00119  |\n",
      "| fps                | 248      |\n",
      "| nupdates           | 600      |\n",
      "| policy_entropy     | 0.626    |\n",
      "| policy_loss        | -482     |\n",
      "| total_timesteps    | 12000    |\n",
      "| value_loss         | 3.97e+06 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=2660.00 +/- 838.09\n",
      "Episode length: 8.20 +/- 0.75\n",
      "Eval num_timesteps=13000, episode_reward=5280.00 +/- 1505.19\n",
      "Episode length: 10.60 +/- 1.36\n",
      "Eval num_timesteps=13500, episode_reward=2020.00 +/- 1999.40\n",
      "Episode length: 8.00 +/- 2.28\n",
      "Eval num_timesteps=14000, episode_reward=5940.00 +/- 3055.88\n",
      "Episode length: 11.80 +/- 3.25\n",
      "---------------------------------\n",
      "| explained_variance | 0.00511  |\n",
      "| fps                | 251      |\n",
      "| nupdates           | 700      |\n",
      "| policy_entropy     | 0.618    |\n",
      "| policy_loss        | -85.1    |\n",
      "| total_timesteps    | 14000    |\n",
      "| value_loss         | 4.46e+06 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=5820.00 +/- 3334.31\n",
      "Episode length: 10.80 +/- 5.04\n",
      "Eval num_timesteps=15000, episode_reward=4520.00 +/- 4453.94\n",
      "Episode length: 11.40 +/- 6.31\n",
      "Eval num_timesteps=15500, episode_reward=5280.00 +/- 5103.49\n",
      "Episode length: 9.60 +/- 7.23\n",
      "Eval num_timesteps=16000, episode_reward=2600.00 +/- 1584.93\n",
      "Episode length: 8.20 +/- 2.32\n",
      "---------------------------------\n",
      "| explained_variance | 0.00175  |\n",
      "| fps                | 252      |\n",
      "| nupdates           | 800      |\n",
      "| policy_entropy     | 0.651    |\n",
      "| policy_loss        | 4.32e+03 |\n",
      "| total_timesteps    | 16000    |\n",
      "| value_loss         | 6.13e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=7420.00 +/- 4061.23\n",
      "Episode length: 13.60 +/- 3.61\n",
      "Eval num_timesteps=17000, episode_reward=4940.00 +/- 5678.94\n",
      "Episode length: 10.40 +/- 6.89\n",
      "Eval num_timesteps=17500, episode_reward=9800.00 +/- 4579.08\n",
      "Episode length: 16.20 +/- 4.79\n",
      "Eval num_timesteps=18000, episode_reward=5400.00 +/- 4502.44\n",
      "Episode length: 11.40 +/- 5.68\n",
      "---------------------------------\n",
      "| explained_variance | -0.00131 |\n",
      "| fps                | 254      |\n",
      "| nupdates           | 900      |\n",
      "| policy_entropy     | 0.639    |\n",
      "| policy_loss        | 4.41e+03 |\n",
      "| total_timesteps    | 18000    |\n",
      "| value_loss         | 5.83e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=5160.00 +/- 3686.52\n",
      "Episode length: 10.60 +/- 3.38\n",
      "Eval num_timesteps=19000, episode_reward=4660.00 +/- 1944.84\n",
      "Episode length: 10.20 +/- 1.94\n",
      "Eval num_timesteps=19500, episode_reward=4920.00 +/- 2050.76\n",
      "Episode length: 10.00 +/- 2.19\n",
      "Eval num_timesteps=20000, episode_reward=5160.00 +/- 1988.57\n",
      "Episode length: 12.00 +/- 2.28\n",
      "---------------------------------\n",
      "| explained_variance | 0.000633 |\n",
      "| fps                | 255      |\n",
      "| nupdates           | 1000     |\n",
      "| policy_entropy     | 0.64     |\n",
      "| policy_loss        | 1.35e+03 |\n",
      "| total_timesteps    | 20000    |\n",
      "| value_loss         | 9.27e+06 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=4920.00 +/- 2844.93\n",
      "Episode length: 11.20 +/- 5.56\n",
      "Eval num_timesteps=21000, episode_reward=4900.00 +/- 2748.09\n",
      "Episode length: 10.60 +/- 3.07\n",
      "Eval num_timesteps=21500, episode_reward=5300.00 +/- 2306.51\n",
      "Episode length: 12.60 +/- 3.20\n",
      "Eval num_timesteps=22000, episode_reward=8540.00 +/- 3107.15\n",
      "Episode length: 17.00 +/- 5.06\n",
      "---------------------------------\n",
      "| explained_variance | 0.000363 |\n",
      "| fps                | 257      |\n",
      "| nupdates           | 1100     |\n",
      "| policy_entropy     | 0.664    |\n",
      "| policy_loss        | 4.27e+03 |\n",
      "| total_timesteps    | 22000    |\n",
      "| value_loss         | 4.97e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=6600.00 +/- 3729.88\n",
      "Episode length: 13.40 +/- 4.96\n",
      "Eval num_timesteps=23000, episode_reward=3600.00 +/- 4055.61\n",
      "Episode length: 8.40 +/- 5.00\n",
      "Eval num_timesteps=23500, episode_reward=5140.00 +/- 2360.17\n",
      "Episode length: 10.40 +/- 3.61\n",
      "Eval num_timesteps=24000, episode_reward=7520.00 +/- 4787.65\n",
      "Episode length: 14.80 +/- 7.03\n",
      "---------------------------------\n",
      "| explained_variance | 0.00767  |\n",
      "| fps                | 258      |\n",
      "| nupdates           | 1200     |\n",
      "| policy_entropy     | 0.681    |\n",
      "| policy_loss        | 3.59e+03 |\n",
      "| total_timesteps    | 24000    |\n",
      "| value_loss         | 3.3e+07  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=24500, episode_reward=6680.00 +/- 5322.93\n",
      "Episode length: 14.80 +/- 7.93\n",
      "Eval num_timesteps=25000, episode_reward=3580.00 +/- 2147.93\n",
      "Episode length: 9.20 +/- 2.79\n",
      "Eval num_timesteps=25500, episode_reward=10700.00 +/- 9046.10\n",
      "Episode length: 17.00 +/- 9.61\n",
      "Eval num_timesteps=26000, episode_reward=5720.00 +/- 2263.98\n",
      "Episode length: 12.40 +/- 3.56\n",
      "---------------------------------\n",
      "| explained_variance | 0.00166  |\n",
      "| fps                | 259      |\n",
      "| nupdates           | 1300     |\n",
      "| policy_entropy     | 0.598    |\n",
      "| policy_loss        | 1.87e+03 |\n",
      "| total_timesteps    | 26000    |\n",
      "| value_loss         | 2.24e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=5400.00 +/- 3824.13\n",
      "Episode length: 13.00 +/- 7.67\n",
      "Eval num_timesteps=27000, episode_reward=2980.00 +/- 1030.34\n",
      "Episode length: 8.60 +/- 1.85\n",
      "Eval num_timesteps=27500, episode_reward=16900.00 +/- 8388.80\n",
      "Episode length: 66.80 +/- 32.99\n",
      "Eval num_timesteps=28000, episode_reward=24400.00 +/- 19665.50\n",
      "Episode length: 38.00 +/- 25.12\n",
      "---------------------------------\n",
      "| explained_variance | 0.000775 |\n",
      "| fps                | 257      |\n",
      "| nupdates           | 1400     |\n",
      "| policy_entropy     | 0.68     |\n",
      "| policy_loss        | 2.69e+03 |\n",
      "| total_timesteps    | 28000    |\n",
      "| value_loss         | 2.02e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=15260.00 +/- 10447.32\n",
      "Episode length: 27.60 +/- 17.61\n",
      "Eval num_timesteps=29000, episode_reward=10180.00 +/- 3108.31\n",
      "Episode length: 22.20 +/- 5.38\n",
      "Eval num_timesteps=29500, episode_reward=6260.00 +/- 7026.69\n",
      "Episode length: 12.80 +/- 8.86\n",
      "Eval num_timesteps=30000, episode_reward=5460.00 +/- 2484.03\n",
      "Episode length: 11.80 +/- 4.02\n",
      "----------------------------------\n",
      "| explained_variance | -0.000837 |\n",
      "| fps                | 257       |\n",
      "| nupdates           | 1500      |\n",
      "| policy_entropy     | 0.595     |\n",
      "| policy_loss        | 2.4e+03   |\n",
      "| total_timesteps    | 30000     |\n",
      "| value_loss         | 3.83e+07  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=21080.00 +/- 7969.79\n",
      "Episode length: 35.80 +/- 11.89\n",
      "Eval num_timesteps=31000, episode_reward=6200.00 +/- 3935.48\n",
      "Episode length: 14.00 +/- 7.16\n",
      "Eval num_timesteps=31500, episode_reward=13520.00 +/- 7544.64\n",
      "Episode length: 28.20 +/- 17.39\n",
      "Eval num_timesteps=32000, episode_reward=13680.00 +/- 7446.18\n",
      "Episode length: 25.60 +/- 12.45\n",
      "----------------------------------\n",
      "| explained_variance | -0.000847 |\n",
      "| fps                | 256       |\n",
      "| nupdates           | 1600      |\n",
      "| policy_entropy     | 0.596     |\n",
      "| policy_loss        | 52        |\n",
      "| total_timesteps    | 32000     |\n",
      "| value_loss         | 4.52e+06  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=6880.00 +/- 4048.90\n",
      "Episode length: 15.20 +/- 9.87\n",
      "Eval num_timesteps=33000, episode_reward=10780.00 +/- 5919.59\n",
      "Episode length: 18.60 +/- 9.07\n",
      "Eval num_timesteps=33500, episode_reward=5040.00 +/- 2603.54\n",
      "Episode length: 11.00 +/- 5.59\n",
      "Eval num_timesteps=34000, episode_reward=13400.00 +/- 11014.35\n",
      "Episode length: 22.40 +/- 15.54\n",
      "---------------------------------\n",
      "| explained_variance | 0.00419  |\n",
      "| fps                | 256      |\n",
      "| nupdates           | 1700     |\n",
      "| policy_entropy     | 0.413    |\n",
      "| policy_loss        | 248      |\n",
      "| total_timesteps    | 34000    |\n",
      "| value_loss         | 1.09e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=14220.00 +/- 13677.34\n",
      "Episode length: 24.60 +/- 22.51\n",
      "Eval num_timesteps=35000, episode_reward=10220.00 +/- 14022.89\n",
      "Episode length: 18.80 +/- 19.97\n",
      "Eval num_timesteps=35500, episode_reward=7380.00 +/- 7061.84\n",
      "Episode length: 13.40 +/- 9.11\n",
      "Eval num_timesteps=36000, episode_reward=7760.00 +/- 8167.64\n",
      "Episode length: 13.60 +/- 12.66\n",
      "---------------------------------\n",
      "| explained_variance | 0.00221  |\n",
      "| fps                | 257      |\n",
      "| nupdates           | 1800     |\n",
      "| policy_entropy     | 0.448    |\n",
      "| policy_loss        | 15       |\n",
      "| total_timesteps    | 36000    |\n",
      "| value_loss         | 5.35e+06 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=28880.00 +/- 16329.04\n",
      "Episode length: 45.60 +/- 28.35\n",
      "New best mean reward!\n",
      "Eval num_timesteps=37000, episode_reward=36700.00 +/- 22770.68\n",
      "Episode length: 66.80 +/- 38.46\n",
      "New best mean reward!\n",
      "Eval num_timesteps=37500, episode_reward=3160.00 +/- 1974.44\n",
      "Episode length: 9.60 +/- 5.12\n",
      "Eval num_timesteps=38000, episode_reward=11580.00 +/- 12184.81\n",
      "Episode length: 20.00 +/- 18.33\n",
      "---------------------------------\n",
      "| explained_variance | -0.00268 |\n",
      "| fps                | 256      |\n",
      "| nupdates           | 1900     |\n",
      "| policy_entropy     | 0.423    |\n",
      "| policy_loss        | 1.18e+03 |\n",
      "| total_timesteps    | 38000    |\n",
      "| value_loss         | 1.55e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=19320.00 +/- 8026.31\n",
      "Episode length: 32.00 +/- 10.97\n",
      "Eval num_timesteps=39000, episode_reward=14640.00 +/- 10062.72\n",
      "Episode length: 28.40 +/- 15.93\n",
      "Eval num_timesteps=39500, episode_reward=23560.00 +/- 20320.10\n",
      "Episode length: 43.40 +/- 31.38\n",
      "Eval num_timesteps=40000, episode_reward=16940.00 +/- 16485.58\n",
      "Episode length: 31.80 +/- 26.13\n",
      "---------------------------------\n",
      "| explained_variance | 0.00236  |\n",
      "| fps                | 254      |\n",
      "| nupdates           | 2000     |\n",
      "| policy_entropy     | 0.463    |\n",
      "| policy_loss        | 1.41e+03 |\n",
      "| total_timesteps    | 40000    |\n",
      "| value_loss         | 1.13e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=20320.00 +/- 17754.14\n",
      "Episode length: 38.00 +/- 29.06\n",
      "Eval num_timesteps=41000, episode_reward=38780.00 +/- 27146.45\n",
      "Episode length: 65.00 +/- 42.87\n",
      "New best mean reward!\n",
      "Eval num_timesteps=41500, episode_reward=36200.00 +/- 24077.71\n",
      "Episode length: 57.00 +/- 32.29\n",
      "Eval num_timesteps=42000, episode_reward=21880.00 +/- 17491.76\n",
      "Episode length: 41.20 +/- 28.17\n",
      "---------------------------------\n",
      "| explained_variance | -0.00277 |\n",
      "| fps                | 252      |\n",
      "| nupdates           | 2100     |\n",
      "| policy_entropy     | 0.539    |\n",
      "| policy_loss        | 1.59e+03 |\n",
      "| total_timesteps    | 42000    |\n",
      "| value_loss         | 1.39e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=13700.00 +/- 10850.81\n",
      "Episode length: 28.40 +/- 17.78\n",
      "Eval num_timesteps=43000, episode_reward=27140.00 +/- 22951.83\n",
      "Episode length: 45.40 +/- 34.00\n",
      "Eval num_timesteps=43500, episode_reward=26640.00 +/- 12650.15\n",
      "Episode length: 43.00 +/- 25.83\n",
      "Eval num_timesteps=44000, episode_reward=26000.00 +/- 19753.79\n",
      "Episode length: 49.40 +/- 36.75\n",
      "---------------------------------\n",
      "| explained_variance | -0.00273 |\n",
      "| fps                | 250      |\n",
      "| nupdates           | 2200     |\n",
      "| policy_entropy     | 0.508    |\n",
      "| policy_loss        | 2.02e+03 |\n",
      "| total_timesteps    | 44000    |\n",
      "| value_loss         | 5.03e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=33860.00 +/- 6077.04\n",
      "Episode length: 59.20 +/- 14.44\n",
      "Eval num_timesteps=45000, episode_reward=28180.00 +/- 19363.00\n",
      "Episode length: 48.20 +/- 27.99\n",
      "Eval num_timesteps=45500, episode_reward=16900.00 +/- 17644.04\n",
      "Episode length: 35.40 +/- 33.37\n",
      "Eval num_timesteps=46000, episode_reward=19740.00 +/- 16432.84\n",
      "Episode length: 38.00 +/- 28.12\n",
      "---------------------------------\n",
      "| explained_variance | 0.00145  |\n",
      "| fps                | 248      |\n",
      "| nupdates           | 2300     |\n",
      "| policy_entropy     | 0.494    |\n",
      "| policy_loss        | 4.13e+03 |\n",
      "| total_timesteps    | 46000    |\n",
      "| value_loss         | 7.98e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=44280.00 +/- 17604.25\n",
      "Episode length: 76.60 +/- 28.32\n",
      "New best mean reward!\n",
      "Eval num_timesteps=47000, episode_reward=38400.00 +/- 23417.26\n",
      "Episode length: 65.60 +/- 36.97\n",
      "Eval num_timesteps=47500, episode_reward=19300.00 +/- 21529.42\n",
      "Episode length: 29.40 +/- 29.29\n",
      "Eval num_timesteps=48000, episode_reward=21060.00 +/- 8646.06\n",
      "Episode length: 39.40 +/- 14.64\n",
      "---------------------------------\n",
      "| explained_variance | 0.00299  |\n",
      "| fps                | 246      |\n",
      "| nupdates           | 2400     |\n",
      "| policy_entropy     | 0.536    |\n",
      "| policy_loss        | 2.57e+03 |\n",
      "| total_timesteps    | 48000    |\n",
      "| value_loss         | 3.4e+07  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=48500, episode_reward=42120.00 +/- 21441.12\n",
      "Episode length: 69.20 +/- 31.98\n",
      "Eval num_timesteps=49000, episode_reward=27460.00 +/- 21754.14\n",
      "Episode length: 46.20 +/- 34.84\n",
      "Eval num_timesteps=49500, episode_reward=19320.00 +/- 18116.22\n",
      "Episode length: 35.40 +/- 28.93\n",
      "Eval num_timesteps=50000, episode_reward=32720.00 +/- 19069.07\n",
      "Episode length: 55.80 +/- 32.27\n",
      "---------------------------------\n",
      "| explained_variance | 0.00791  |\n",
      "| fps                | 244      |\n",
      "| nupdates           | 2500     |\n",
      "| policy_entropy     | 0.456    |\n",
      "| policy_loss        | 2.38e+03 |\n",
      "| total_timesteps    | 50000    |\n",
      "| value_loss         | 3.14e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=24960.00 +/- 26461.94\n",
      "Episode length: 46.60 +/- 41.82\n",
      "Eval num_timesteps=51000, episode_reward=37800.00 +/- 20264.55\n",
      "Episode length: 68.40 +/- 33.77\n",
      "Eval num_timesteps=51500, episode_reward=31160.00 +/- 22204.47\n",
      "Episode length: 52.80 +/- 38.26\n",
      "Eval num_timesteps=52000, episode_reward=42940.00 +/- 16385.80\n",
      "Episode length: 74.80 +/- 24.03\n",
      "---------------------------------\n",
      "| explained_variance | -0.00127 |\n",
      "| fps                | 240      |\n",
      "| nupdates           | 2600     |\n",
      "| policy_entropy     | 0.402    |\n",
      "| policy_loss        | 1.47e+03 |\n",
      "| total_timesteps    | 52000    |\n",
      "| value_loss         | 1.92e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=41440.00 +/- 18487.03\n",
      "Episode length: 69.60 +/- 28.71\n",
      "Eval num_timesteps=53000, episode_reward=18980.00 +/- 18175.30\n",
      "Episode length: 34.80 +/- 26.47\n",
      "Eval num_timesteps=53500, episode_reward=12340.00 +/- 13291.29\n",
      "Episode length: 26.60 +/- 25.55\n",
      "Eval num_timesteps=54000, episode_reward=20580.00 +/- 10584.40\n",
      "Episode length: 39.80 +/- 23.12\n",
      "---------------------------------\n",
      "| explained_variance | 0.00352  |\n",
      "| fps                | 240      |\n",
      "| nupdates           | 2700     |\n",
      "| policy_entropy     | 0.344    |\n",
      "| policy_loss        | 483      |\n",
      "| total_timesteps    | 54000    |\n",
      "| value_loss         | 6.54e+06 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=28560.00 +/- 29428.53\n",
      "Episode length: 46.40 +/- 43.86\n",
      "Eval num_timesteps=55000, episode_reward=6260.00 +/- 4553.06\n",
      "Episode length: 12.80 +/- 8.28\n",
      "Eval num_timesteps=55500, episode_reward=32240.00 +/- 14611.45\n",
      "Episode length: 56.40 +/- 24.69\n",
      "Eval num_timesteps=56000, episode_reward=14860.00 +/- 13129.30\n",
      "Episode length: 28.20 +/- 20.43\n",
      "---------------------------------\n",
      "| explained_variance | 0.00874  |\n",
      "| fps                | 239      |\n",
      "| nupdates           | 2800     |\n",
      "| policy_entropy     | 0.292    |\n",
      "| policy_loss        | 1.26e+03 |\n",
      "| total_timesteps    | 56000    |\n",
      "| value_loss         | 2.12e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=26380.00 +/- 18692.82\n",
      "Episode length: 40.00 +/- 30.71\n",
      "Eval num_timesteps=57000, episode_reward=23060.00 +/- 15969.04\n",
      "Episode length: 40.40 +/- 24.66\n",
      "Eval num_timesteps=57500, episode_reward=25920.00 +/- 18853.16\n",
      "Episode length: 49.60 +/- 35.52\n",
      "Eval num_timesteps=58000, episode_reward=20020.00 +/- 16406.51\n",
      "Episode length: 31.60 +/- 23.80\n",
      "---------------------------------\n",
      "| explained_variance | 0.00197  |\n",
      "| fps                | 238      |\n",
      "| nupdates           | 2900     |\n",
      "| policy_entropy     | 0.41     |\n",
      "| policy_loss        | 2.34e+03 |\n",
      "| total_timesteps    | 58000    |\n",
      "| value_loss         | 7.86e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=25460.00 +/- 21921.46\n",
      "Episode length: 41.20 +/- 33.84\n",
      "Eval num_timesteps=59000, episode_reward=24840.00 +/- 20757.32\n",
      "Episode length: 45.80 +/- 32.61\n",
      "Eval num_timesteps=59500, episode_reward=10460.00 +/- 6915.66\n",
      "Episode length: 20.00 +/- 10.04\n",
      "Eval num_timesteps=60000, episode_reward=6920.00 +/- 5948.92\n",
      "Episode length: 14.40 +/- 8.31\n",
      "---------------------------------\n",
      "| explained_variance | 0.00427  |\n",
      "| fps                | 238      |\n",
      "| nupdates           | 3000     |\n",
      "| policy_entropy     | 0.279    |\n",
      "| policy_loss        | 989      |\n",
      "| total_timesteps    | 60000    |\n",
      "| value_loss         | 3.04e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=24440.00 +/- 15715.29\n",
      "Episode length: 45.40 +/- 29.62\n",
      "Eval num_timesteps=61000, episode_reward=27540.00 +/- 22757.12\n",
      "Episode length: 51.40 +/- 36.12\n",
      "Eval num_timesteps=61500, episode_reward=13860.00 +/- 10358.49\n",
      "Episode length: 30.40 +/- 20.85\n",
      "Eval num_timesteps=62000, episode_reward=36300.00 +/- 20212.57\n",
      "Episode length: 64.80 +/- 32.20\n",
      "---------------------------------\n",
      "| explained_variance | 0.00686  |\n",
      "| fps                | 236      |\n",
      "| nupdates           | 3100     |\n",
      "| policy_entropy     | 0.44     |\n",
      "| policy_loss        | 879      |\n",
      "| total_timesteps    | 62000    |\n",
      "| value_loss         | 2.38e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=36300.00 +/- 23680.20\n",
      "Episode length: 62.40 +/- 36.19\n",
      "Eval num_timesteps=63000, episode_reward=26780.00 +/- 13659.19\n",
      "Episode length: 49.60 +/- 25.94\n",
      "Eval num_timesteps=63500, episode_reward=22020.00 +/- 15328.07\n",
      "Episode length: 42.40 +/- 29.30\n",
      "Eval num_timesteps=64000, episode_reward=27260.00 +/- 16116.40\n",
      "Episode length: 44.20 +/- 23.49\n",
      "---------------------------------\n",
      "| explained_variance | 0.000929 |\n",
      "| fps                | 236      |\n",
      "| nupdates           | 3200     |\n",
      "| policy_entropy     | 0.484    |\n",
      "| policy_loss        | 197      |\n",
      "| total_timesteps    | 64000    |\n",
      "| value_loss         | 1e+07    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=39940.00 +/- 26616.05\n",
      "Episode length: 65.00 +/- 42.87\n",
      "Eval num_timesteps=65000, episode_reward=36860.00 +/- 15332.14\n",
      "Episode length: 61.40 +/- 26.69\n",
      "Eval num_timesteps=65500, episode_reward=42580.00 +/- 21235.48\n",
      "Episode length: 71.80 +/- 32.68\n",
      "Eval num_timesteps=66000, episode_reward=29940.00 +/- 23056.94\n",
      "Episode length: 48.00 +/- 34.67\n",
      "----------------------------------\n",
      "| explained_variance | -0.000524 |\n",
      "| fps                | 234       |\n",
      "| nupdates           | 3300      |\n",
      "| policy_entropy     | 0.391     |\n",
      "| policy_loss        | 5.1e+03   |\n",
      "| total_timesteps    | 66000     |\n",
      "| value_loss         | 6.42e+07  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=15480.00 +/- 8045.47\n",
      "Episode length: 27.80 +/- 11.29\n",
      "Eval num_timesteps=67000, episode_reward=28820.00 +/- 19329.29\n",
      "Episode length: 47.80 +/- 30.98\n",
      "Eval num_timesteps=67500, episode_reward=21440.00 +/- 23941.23\n",
      "Episode length: 35.00 +/- 36.86\n",
      "Eval num_timesteps=68000, episode_reward=28940.00 +/- 19511.90\n",
      "Episode length: 49.60 +/- 29.40\n",
      "---------------------------------\n",
      "| explained_variance | 0.000484 |\n",
      "| fps                | 233      |\n",
      "| nupdates           | 3400     |\n",
      "| policy_entropy     | 0.262    |\n",
      "| policy_loss        | 1.09e+03 |\n",
      "| total_timesteps    | 68000    |\n",
      "| value_loss         | 1.22e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=25180.00 +/- 29546.94\n",
      "Episode length: 44.00 +/- 45.01\n",
      "Eval num_timesteps=69000, episode_reward=31740.00 +/- 21856.13\n",
      "Episode length: 58.60 +/- 40.91\n",
      "Eval num_timesteps=69500, episode_reward=11500.00 +/- 8850.31\n",
      "Episode length: 20.80 +/- 15.38\n",
      "Eval num_timesteps=70000, episode_reward=23400.00 +/- 20995.81\n",
      "Episode length: 37.40 +/- 32.96\n",
      "---------------------------------\n",
      "| explained_variance | -0.0036  |\n",
      "| fps                | 231      |\n",
      "| nupdates           | 3500     |\n",
      "| policy_entropy     | 0.345    |\n",
      "| policy_loss        | 334      |\n",
      "| total_timesteps    | 70000    |\n",
      "| value_loss         | 7.93e+06 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70500, episode_reward=13100.00 +/- 13235.10\n",
      "Episode length: 23.20 +/- 21.68\n",
      "Eval num_timesteps=71000, episode_reward=9720.00 +/- 10632.67\n",
      "Episode length: 20.00 +/- 17.73\n",
      "Eval num_timesteps=71500, episode_reward=25900.00 +/- 22108.10\n",
      "Episode length: 42.60 +/- 34.09\n",
      "Eval num_timesteps=72000, episode_reward=31680.00 +/- 27179.43\n",
      "Episode length: 52.00 +/- 40.42\n",
      "---------------------------------\n",
      "| explained_variance | -0.00028 |\n",
      "| fps                | 228      |\n",
      "| nupdates           | 3600     |\n",
      "| policy_entropy     | 0.403    |\n",
      "| policy_loss        | 2.66e+03 |\n",
      "| total_timesteps    | 72000    |\n",
      "| value_loss         | 4.45e+07 |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=72500, episode_reward=15520.00 +/- 18625.72\n",
      "Episode length: 27.20 +/- 26.72\n",
      "Eval num_timesteps=73000, episode_reward=11220.00 +/- 11435.80\n",
      "Episode length: 20.80 +/- 17.86\n",
      "Eval num_timesteps=73500, episode_reward=20480.00 +/- 12835.17\n",
      "Episode length: 35.20 +/- 19.08\n",
      "Eval num_timesteps=74000, episode_reward=19540.00 +/- 13926.32\n",
      "Episode length: 35.80 +/- 22.79\n",
      "---------------------------------\n",
      "| explained_variance | 0.00357  |\n",
      "| fps                | 227      |\n",
      "| nupdates           | 3700     |\n",
      "| policy_entropy     | 0.276    |\n",
      "| policy_loss        | 533      |\n",
      "| total_timesteps    | 74000    |\n",
      "| value_loss         | 8.54e+06 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=35920.00 +/- 23099.04\n",
      "Episode length: 60.60 +/- 34.81\n",
      "Eval num_timesteps=75000, episode_reward=31560.00 +/- 22288.71\n",
      "Episode length: 57.80 +/- 41.47\n",
      "Eval num_timesteps=75500, episode_reward=32480.00 +/- 18574.00\n",
      "Episode length: 54.80 +/- 30.37\n",
      "Eval num_timesteps=76000, episode_reward=12280.00 +/- 8315.86\n",
      "Episode length: 22.00 +/- 13.22\n",
      "---------------------------------\n",
      "| explained_variance | 0.00464  |\n",
      "| fps                | 225      |\n",
      "| nupdates           | 3800     |\n",
      "| policy_entropy     | 0.378    |\n",
      "| policy_loss        | 2.37e+03 |\n",
      "| total_timesteps    | 76000    |\n",
      "| value_loss         | 2.04e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=46140.00 +/- 21950.54\n",
      "Episode length: 76.20 +/- 36.04\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77000, episode_reward=14100.00 +/- 13813.04\n",
      "Episode length: 29.40 +/- 22.93\n",
      "Eval num_timesteps=77500, episode_reward=40840.00 +/- 21683.60\n",
      "Episode length: 68.40 +/- 35.48\n",
      "Eval num_timesteps=78000, episode_reward=14980.00 +/- 13332.58\n",
      "Episode length: 27.80 +/- 22.52\n",
      "---------------------------------\n",
      "| explained_variance | 0.00488  |\n",
      "| fps                | 222      |\n",
      "| nupdates           | 3900     |\n",
      "| policy_entropy     | 0.338    |\n",
      "| policy_loss        | 890      |\n",
      "| total_timesteps    | 78000    |\n",
      "| value_loss         | 6.14e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=12400.00 +/- 4656.18\n",
      "Episode length: 24.80 +/- 8.21\n",
      "Eval num_timesteps=79000, episode_reward=23360.00 +/- 16548.06\n",
      "Episode length: 38.80 +/- 24.19\n",
      "Eval num_timesteps=79500, episode_reward=15200.00 +/- 9624.97\n",
      "Episode length: 30.20 +/- 19.11\n",
      "Eval num_timesteps=80000, episode_reward=36860.00 +/- 13744.90\n",
      "Episode length: 63.20 +/- 23.98\n",
      "---------------------------------\n",
      "| explained_variance | 0.00756  |\n",
      "| fps                | 220      |\n",
      "| nupdates           | 4000     |\n",
      "| policy_entropy     | 0.274    |\n",
      "| policy_loss        | 1.69e+03 |\n",
      "| total_timesteps    | 80000    |\n",
      "| value_loss         | 2.24e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80500, episode_reward=23000.00 +/- 20540.01\n",
      "Episode length: 42.00 +/- 34.15\n",
      "Eval num_timesteps=81000, episode_reward=43180.00 +/- 15613.38\n",
      "Episode length: 73.40 +/- 24.49\n",
      "Eval num_timesteps=81500, episode_reward=39120.00 +/- 12420.85\n",
      "Episode length: 67.40 +/- 23.10\n",
      "Eval num_timesteps=82000, episode_reward=22860.00 +/- 21481.96\n",
      "Episode length: 44.20 +/- 34.06\n",
      "---------------------------------\n",
      "| explained_variance | -0.00679 |\n",
      "| fps                | 218      |\n",
      "| nupdates           | 4100     |\n",
      "| policy_entropy     | 0.308    |\n",
      "| policy_loss        | 1.07e+03 |\n",
      "| total_timesteps    | 82000    |\n",
      "| value_loss         | 1.19e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82500, episode_reward=35520.00 +/- 22847.88\n",
      "Episode length: 64.00 +/- 37.41\n",
      "Eval num_timesteps=83000, episode_reward=17360.00 +/- 16678.56\n",
      "Episode length: 28.40 +/- 22.62\n",
      "Eval num_timesteps=83500, episode_reward=30040.00 +/- 21224.19\n",
      "Episode length: 54.00 +/- 33.92\n",
      "Eval num_timesteps=84000, episode_reward=20400.00 +/- 12864.21\n",
      "Episode length: 35.20 +/- 18.30\n",
      "---------------------------------\n",
      "| explained_variance | -0.00691 |\n",
      "| fps                | 216      |\n",
      "| nupdates           | 4200     |\n",
      "| policy_entropy     | 0.396    |\n",
      "| policy_loss        | 2.45e+03 |\n",
      "| total_timesteps    | 84000    |\n",
      "| value_loss         | 3.11e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84500, episode_reward=21120.00 +/- 10329.26\n",
      "Episode length: 38.40 +/- 18.87\n",
      "Eval num_timesteps=85000, episode_reward=15180.00 +/- 19530.12\n",
      "Episode length: 29.80 +/- 36.52\n",
      "Eval num_timesteps=85500, episode_reward=28500.00 +/- 25493.92\n",
      "Episode length: 48.20 +/- 41.95\n",
      "Eval num_timesteps=86000, episode_reward=27520.00 +/- 20280.96\n",
      "Episode length: 48.40 +/- 33.41\n",
      "---------------------------------\n",
      "| explained_variance | 0.0032   |\n",
      "| fps                | 214      |\n",
      "| nupdates           | 4300     |\n",
      "| policy_entropy     | 0.332    |\n",
      "| policy_loss        | -799     |\n",
      "| total_timesteps    | 86000    |\n",
      "| value_loss         | 1.05e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86500, episode_reward=38000.00 +/- 29354.52\n",
      "Episode length: 63.00 +/- 43.90\n",
      "Eval num_timesteps=87000, episode_reward=13540.00 +/- 6959.77\n",
      "Episode length: 26.40 +/- 12.74\n",
      "Eval num_timesteps=87500, episode_reward=22340.00 +/- 17106.56\n",
      "Episode length: 43.40 +/- 30.22\n",
      "Eval num_timesteps=88000, episode_reward=15420.00 +/- 15402.26\n",
      "Episode length: 25.80 +/- 23.04\n",
      "---------------------------------\n",
      "| explained_variance | 0.00225  |\n",
      "| fps                | 212      |\n",
      "| nupdates           | 4400     |\n",
      "| policy_entropy     | 0.387    |\n",
      "| policy_loss        | 1.75e+03 |\n",
      "| total_timesteps    | 88000    |\n",
      "| value_loss         | 3.25e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88500, episode_reward=34620.00 +/- 20650.37\n",
      "Episode length: 61.80 +/- 31.01\n",
      "Eval num_timesteps=89000, episode_reward=17460.00 +/- 14919.60\n",
      "Episode length: 32.60 +/- 21.56\n",
      "Eval num_timesteps=89500, episode_reward=8840.00 +/- 7864.25\n",
      "Episode length: 18.20 +/- 15.01\n",
      "Eval num_timesteps=90000, episode_reward=35740.00 +/- 20924.68\n",
      "Episode length: 60.00 +/- 31.56\n",
      "---------------------------------\n",
      "| explained_variance | -0.00493 |\n",
      "| fps                | 210      |\n",
      "| nupdates           | 4500     |\n",
      "| policy_entropy     | 0.351    |\n",
      "| policy_loss        | 1.58e+03 |\n",
      "| total_timesteps    | 90000    |\n",
      "| value_loss         | 5.15e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90500, episode_reward=25060.00 +/- 19744.12\n",
      "Episode length: 43.40 +/- 31.22\n",
      "Eval num_timesteps=91000, episode_reward=24380.00 +/- 22363.40\n",
      "Episode length: 42.20 +/- 31.83\n",
      "Eval num_timesteps=91500, episode_reward=35180.00 +/- 17026.73\n",
      "Episode length: 60.60 +/- 25.78\n",
      "Eval num_timesteps=92000, episode_reward=22180.00 +/- 22419.94\n",
      "Episode length: 34.60 +/- 32.92\n",
      "---------------------------------\n",
      "| explained_variance | -0.00923 |\n",
      "| fps                | 209      |\n",
      "| nupdates           | 4600     |\n",
      "| policy_entropy     | 0.302    |\n",
      "| policy_loss        | 1.1e+03  |\n",
      "| total_timesteps    | 92000    |\n",
      "| value_loss         | 4.14e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92500, episode_reward=23480.00 +/- 21190.22\n",
      "Episode length: 41.00 +/- 31.63\n",
      "Eval num_timesteps=93000, episode_reward=26040.00 +/- 19245.42\n",
      "Episode length: 44.80 +/- 28.02\n",
      "Eval num_timesteps=93500, episode_reward=49960.00 +/- 16266.60\n",
      "Episode length: 80.80 +/- 23.65\n",
      "New best mean reward!\n",
      "Eval num_timesteps=94000, episode_reward=14880.00 +/- 20876.72\n",
      "Episode length: 27.40 +/- 33.90\n",
      "---------------------------------\n",
      "| explained_variance | -0.00494 |\n",
      "| fps                | 207      |\n",
      "| nupdates           | 4700     |\n",
      "| policy_entropy     | 0.329    |\n",
      "| policy_loss        | 2.34e+03 |\n",
      "| total_timesteps    | 94000    |\n",
      "| value_loss         | 3.53e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=94500, episode_reward=28040.00 +/- 5520.00\n",
      "Episode length: 49.00 +/- 12.00\n",
      "Eval num_timesteps=95000, episode_reward=21780.00 +/- 15882.75\n",
      "Episode length: 35.60 +/- 28.09\n",
      "Eval num_timesteps=95500, episode_reward=16780.00 +/- 9256.65\n",
      "Episode length: 26.60 +/- 13.50\n",
      "Eval num_timesteps=96000, episode_reward=31260.00 +/- 19122.51\n",
      "Episode length: 53.60 +/- 32.27\n",
      "---------------------------------\n",
      "| explained_variance | 0.00456  |\n",
      "| fps                | 205      |\n",
      "| nupdates           | 4800     |\n",
      "| policy_entropy     | 0.233    |\n",
      "| policy_loss        | 697      |\n",
      "| total_timesteps    | 96000    |\n",
      "| value_loss         | 3.31e+07 |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=96500, episode_reward=31160.00 +/- 19014.90\n",
      "Episode length: 57.60 +/- 31.54\n",
      "Eval num_timesteps=97000, episode_reward=36620.00 +/- 22728.26\n",
      "Episode length: 62.40 +/- 28.54\n",
      "Eval num_timesteps=97500, episode_reward=19340.00 +/- 15981.81\n",
      "Episode length: 38.80 +/- 26.82\n",
      "Eval num_timesteps=98000, episode_reward=8900.00 +/- 10356.06\n",
      "Episode length: 17.20 +/- 15.60\n",
      "---------------------------------\n",
      "| explained_variance | 0.00061  |\n",
      "| fps                | 204      |\n",
      "| nupdates           | 4900     |\n",
      "| policy_entropy     | 0.321    |\n",
      "| policy_loss        | 1.71e+03 |\n",
      "| total_timesteps    | 98000    |\n",
      "| value_loss         | 4.94e+07 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98500, episode_reward=21540.00 +/- 8188.19\n",
      "Episode length: 38.80 +/- 15.41\n",
      "Eval num_timesteps=99000, episode_reward=30680.00 +/- 19155.72\n",
      "Episode length: 55.60 +/- 32.20\n",
      "Eval num_timesteps=99500, episode_reward=20900.00 +/- 15331.27\n",
      "Episode length: 39.00 +/- 31.32\n",
      "Eval num_timesteps=100000, episode_reward=30140.00 +/- 19862.79\n",
      "Episode length: 50.80 +/- 29.27\n",
      "---------------------------------\n",
      "| explained_variance | -0.00417 |\n",
      "| fps                | 202      |\n",
      "| nupdates           | 5000     |\n",
      "| policy_entropy     | 0.206    |\n",
      "| policy_loss        | -474     |\n",
      "| total_timesteps    | 100000   |\n",
      "| value_loss         | 6.31e+06 |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(act_fun=tf.nn.relu, net_arch=[dict(vf=[128,64], pi=[128,64])]) #no shared layer, actor and critic have individual NN\n",
    "eval_callback2 = EvalCallback(eval_env, eval_freq=500,\n",
    "                             deterministic=True, render=False)\n",
    "model2 = ACKTR('MlpPolicy',env,verbose=1,policy_kwargs=policy_kwargs).learn(total_timesteps=100000,callback=eval_callback2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e55c2f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'model/pi_fc0/w:0' shape=(4, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'model/pi_fc0/b:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'model/vf_fc0/w:0' shape=(4, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'model/vf_fc0/b:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'model/pi_fc1/w:0' shape=(128, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'model/pi_fc1/b:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'model/vf_fc1/w:0' shape=(128, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'model/vf_fc1/b:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'model/vf/w:0' shape=(64, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'model/vf/b:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'model/pi/w:0' shape=(64, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'model/pi/b:0' shape=(2,) dtype=float32_ref>,\n",
       " <tf.Variable 'model/q/w:0' shape=(64, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'model/q/b:0' shape=(2,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f0cd41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
